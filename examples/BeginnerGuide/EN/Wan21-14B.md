# Trying T2V and I2V with Wan21-14B

This document contains usage examples for the Wan2.1-T2V-14B and Wan2.1-I2V-14B-480P / Wan2.1-I2V-14B-720P models.

## Prepare the environment

Please refer to [01.PrepareEnv](01.PrepareEnv.md)

## Getting started

Prepare the models
```
# Download from Hugging Face
hf download Wan-AI/Wan2.1-T2V-14B --local-dir Wan-AI/Wan2.1-T2V-14B
hf download Wan-AI/Wan2.1-I2V-14B-480P --local-dir Wan-AI/Wan2.1-I2V-14B-480P
hf download Wan-AI/Wan2.1-I2V-14B-720P --local-dir Wan-AI/Wan2.1-I2V-14B-720P

# Download distillation models
hf download lightx2v/Wan2.1-Distill-Models --local-dir lightx2v/Wan2.1-Distill-Models
hf download lightx2v/Wan2.1-Distill-Loras --local-dir lightx2v/Wan2.1-Distill-Loras
```
We provide three ways to run the Wan21-14B models to generate videos:

1. Run the provided scripts: pre-set bash scripts for quick verification.
2. Start a server and send requests: suitable for repeated inference and production deployment.
3. Use Python code: easy to integrate into existing codebases.

### Run using scripts

```
git clone https://github.com/ModelTC/LightX2V.git

# Before running the scripts below, replace `lightx2v_path` and `model_path` in the scripts with actual paths
# e.g.: lightx2v_path=/home/user/LightX2V
# e.g.: model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
```

Wan2.1-T2V-14B model
```
# Replace CUDA_VISIBLE_DEVICES with actual GPU IDs before running
# Also adjust the `parallel` parameters in the config file so that cfg_p_size * seq_p_size = number of GPUs
cd LightX2V/scripts/dist_infer
bash run_wan_t2v_dist_cfg_ulysses.sh

# Distillation model (LoRA steps)
cd LightX2V/scripts/wan
bash run_wan_t2v_distill_lora_4step_cfg_ulysses.sh

# Distillation model (merged LoRA)
cd LightX2V/scripts/wan
bash run_wan_t2v_distill_model_4step_cfg_ulysses.sh

# Distillation + FP8 quantized model
cd LightX2V/scripts/wan
bash run_wan_t2v_distill_fp8_4step_cfg_ulysses.sh
```
Note: the `model_path` for distillation models should point to the original model path; put the distillation model path in the config_json.

Wan2.1-I2V-14B model
```
# Switch `model_path` and `config_json` to try Wan2.1-I2V-14B-480P or Wan2.1-I2V-14B-720P
cd LightX2V/scripts/dist_infer
bash run_wan_i2v_dist_cfg_ulysses.sh

# Distillation model (LoRA steps)
cd LightX2V/scripts/wan
bash run_wan_i2v_distill_lora_4step_cfg_ulysses.sh

# Distillation model (merged LoRA)
cd LightX2V/scripts/wan
bash run_wan_i2v_distill_model_4step_cfg_ulysses.sh

# Distillation + FP8 quantized model
cd LightX2V/scripts/wan
bash run_wan_i2v_distill_fp8_4step_cfg_ulysses.sh
```

Details explained

The `run_wan_t2v_dist_cfg_ulysses.sh` script is as follows:
```
#!/bin/bash

# set path firstly
lightx2v_path=
model_path=

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# set environment variables
source ${lightx2v_path}/scripts/base/base.sh

torchrun --nproc_per_node=8 -m lightx2v.infer \
--model_cls wan2.1 \
--task t2v \
--model_path $model_path \
--config_json ${lightx2v_path}/configs/dist_infer/wan_t2v_dist_cfg_ulysses.json \
--prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage." \
--negative_prompt "camera shake, vivid color tones, overexposure, static, blurred details, subtitles, style marks, artwork, painting-like, still image, overall grayish, worst quality, low quality, JPEG compression artifacts, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fused fingers, motionless frame, cluttered background, three legs, many people in background, walking backwards" \
--save_result_path ${lightx2v_path}/save_results/output_lightx2v_wan_t2v.mp4

```
`export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7` means using GPUs 0â€“7 (8 GPUs total).

`torchrun --nproc_per_node=8 -m lightx2v.infer` starts multi-GPU inference with `torchrun`, launching 8 processes (one per GPU).

The `wan_t2v_dist_cfg_ulysses.json` content:
```
{
    "infer_steps": 50,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 8,
    "enable_cfg": true,
    "cpu_offload": false,
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
}

```
`cpu_offload` indicates whether CPU offload is enabled. Enabling CPU offload can reduce GPU memory usage; if enabled, add `"offload_granularity": "block"` to offload by DiT Transformer blocks. After enabling, use `watch -n 1 nvidia-smi` to monitor GPU memory usage.

`parallel` configures parallel settings. DiT supports two parallel attention modes: Ulysses and Ring, and also supports CFG parallel inference. Parallel inference significantly reduces inference time and per-GPU memory usage.

`wan_t2v_distill_lora_4step_cfg_ulysses.json` content:
```
{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 5,
    "enable_cfg": false,
    "cpu_offload": false,
    "denoising_step_list": [1000, 750, 500, 250],
    "lora_configs": [
      {
        "path": "lightx2v/Wan2.1-Distill-Loras/wan2.1_t2v_14b_lora_rank64_lightx2v_4step.safetensors",
        "strength": 1.0
      }
    ],
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
  }

```
`denoising_step_list` indicates the denoising strengths for the 4-step inference.

`lora_configs` is the LoRA plugin configuration; specify the distillation LoRA path.

`wan_t2v_distill_model_4step_cfg_ulysses.json` content:
```
{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 5,
    "enable_cfg": false,
    "cpu_offload": false,
    "denoising_step_list": [1000, 750, 500, 250],
    "dit_original_ckpt": "lightx2v/Wan2.1-Distill-Models/wan2.1_t2v_14b_lightx2v_4step.safetensors",
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
}

```
`dit_original_ckpt` is the path to the distillation model after merging LoRA.

`wan_t2v_distill_fp8_4step_cfg_ulysses.json` content:
```
{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 5,
    "enable_cfg": false,
    "cpu_offload": false,
    "denoising_step_list": [1000, 750, 500, 250],
    "dit_quantized": true,
    "dit_quantized_ckpt": "lightx2v/Wan2.1-Distill-Models/wan2.1_t2v_14b_scaled_fp8_e4m3_lightx2v_4step.safetensors",
    "dit_quant_scheme": "fp8-sgl",
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
}

```
`dit_quantized_ckpt` specifies the local path to the quantized DIT weights (FP8).

`dit_quant_scheme` specifies the DIT quantization scheme; here "fp8-sgl" indicates using the sglang FP8 kernel for inference.

### Start server mode

Start the server
```
cd LightX2V/scripts/server

# Before running the script below, replace `lightx2v_path` and `model_path` in the script with actual paths
# e.g.: lightx2v_path=/home/user/LightX2V
# e.g.: model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
# Also: config_json should point to the corresponding model config
# e.g.: config_json ${lightx2v_path}/configs/wan/wan_t2v.json

# Switch `model_path` and `config_json` to try different models
bash start_server.sh
```
Send requests to the server

Open a second terminal as the client
```
cd LightX2V/scripts/server

# The generated video endpoint is "http://localhost:8000/v1/tasks/video/"
python post.py
```
After sending the request, you can see inference logs on the server side.

### Generate via Python code

```
cd LightX2V/examples/wan/wan_t2v.py

# Modify `model_path` and `save_result_path`
# Option 1: multi-GPU
# Add the two lines below and run with torchrun to start multi-GPU
# import os
# os.environ["PROFILING_DEBUG_LEVEL"] = "2"
# PYTHONPATH=/home/user/LightX2V torchrun --nproc_per_node=8 wan_t2v.py

# Option 2: single GPU
python wan_t2v.py
```
Note: In Set runtime parameters, it is recommended to use the config_json method to align hyperparameters with the previous script-based and service-based video generation methods.
