# Experience T2V and I2V from Wan21-14B

This document contains usage examples for Wan2.1-T2V-14B and Wan2.1-I2V-14B-480P, Wan2.1-I2V-14B-720P models.

## Prepare Environment

Please refer to [01.PrepareEnv](01.PrepareEnv.md)

## Start Running

Prepare Models
```
# Download from HuggingFace
hf download Wan-AI/Wan2.1-T2V-14B --local-dir Wan-AI/Wan2.1-T2V-14B
hf download Wan-AI/Wan2.1-I2V-14B-480P --local-dir Wan-AI/Wan2.1-I2V-14B-480P
hf download Wan-AI/Wan2.1-I2V-14B-720P --local-dir Wan-AI/Wan2.1-I2V-14B-720P

# Download distilled models
hf download lightx2v/Wan2.1-Distill-Models --local-dir lightx2v/Wan2.1-Distill-Models
hf download lightx2v/Wan2.1-Distill-Loras --local-dir lightx2v/Wan2.1-Distill-Loras
```
We provide three ways to run the Wan21-14B model to generate videos:

1. Run Script Generation: Preset bash scripts that can be run directly for quick verification

    1.1 Single GPU Inference

    1.2 Single GPU Offload Inference

    1.3 Multi-GPU Parallel Inference

2. Start Service Generation: Start the service first, then send requests, suitable for multiple inferences and actual online deployment

    2.1 Single GPU Inference

    2.2 Single GPU Offload Inference

    2.3 Multi-GPU Parallel Inference

3. Python Code Generation: Run with Python code, convenient for integration into existing code environments

    3.1 Single GPU Inference

    3.2 Single GPU Offload Inference

    3.3 Multi-GPU Parallel Inference


### 1. Run Script Generation

```
git clone https://github.com/ModelTC/LightX2V.git

# Before running the following scripts, you need to replace lightx2v_path and model_path in the scripts with actual paths
# For example: lightx2v_path=/home/user/LightX2V
# For example: model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
```

#### 1.1 Single GPU Inference

Wan2.1-T2V-14B Model
```
# model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B

cd LightX2V/scripts/wan
bash run_wan_t2v.sh

# Step-Distilled Model with LoRA
# model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
cd LightX2V/scripts/wan/distill
bash run_wan_t2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
# model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
bash run_wan_t2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
# model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
bash run_wan_t2v_distill_fp8_4step_cfg.sh
```
Note: The model_path in the bash script refers to the path of the pre-trained original model; the lora_configs, dit_original_ckpt and dit_quantized_ckpt in the config file are the paths to the distilled models used, and need to be modified to absolute paths, for example: /home/user/models/lightx2v/Wan2.1-Distill-Models/wan2.1_i2v_480p_int8_lightx2v_4step.safetensors

Using a single H100 GPU, the runtime and peak VRAM usage (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Wan2.1-T2V-14B model: Total Cost 278.902019 seconds; 43768MiB
2. Step-Distilled Model with LoRA: Total Cost 31.365923 seconds; 44438MiB
3. Step-Distilled Model with Merged LoRA: Total Cost 25.794410 seconds; 44418MiB
4. Step-Distilled + FP8 Quantization Model: Total Cost 22.000187 seconds; 31032MiB

Wan2.1-I2V-14B Model
```
# Switch model_path and config_json to experience Wan2.1-I2V-14B-480P and Wan2.1-I2V-14B-720P
cd LightX2V/scripts/wan
bash run_wan_i2v.sh

# Step-Distilled Model with LoRA
cd LightX2V/scripts/wan/distill
bash run_wan_i2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_i2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_i2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_i2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_i2v_distill_fp8_4step_cfg.sh
```
Using a single H100 GPU, the runtime and peak VRAM usage (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Wan2.1-I2V-14B-480P model: Total Cost 232.971375 seconds; 49872MiB
2. Step-Distilled Model with LoRA: Total Cost 277.535991 seconds; 49782MiB
3. Step-Distilled Model with Merged LoRA: Total Cost 26.841140 seconds; 49526MiB
4. Step-Distilled + FP8 Quantization Model: Total Cost 25.430433 seconds; 34218MiB

#### 1.2 Single GPU Offload Inference

Modify the cpu_offload in the config file as follows to enable offload:
```
    "cpu_offload": true,
    "offload_granularity": "model"
```

Wan2.1-T2V-14B Model
```
cd LightX2V/scripts/wan
bash run_wan_t2v.sh

# Step-Distilled Model with LoRA
cd LightX2V/scripts/wan/distill
bash run_wan_t2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_t2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_t2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_t2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_t2v_distill_fp8_4step_cfg.sh
```
Using a single H100 GPU, the runtime and peak VRAM usage (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Wan2.1-T2V-14B model: Total Cost 319.019743 seconds; 34932MiB
2. Step-Distilled Model with LoRA: Total Cost 74.180393 seconds; 34562MiB
3. Step-Distilled Model with Merged LoRA: Total Cost 68.621963 seconds; 34562MiB
4. Step-Distilled + FP8 Quantization Model: Total Cost 58.921504 seconds; 21290MiB

Wan2.1-I2V-14B Model
```
# Switch model_path and config_json to experience Wan2.1-I2V-14B-480P and Wan2.1-I2V-14B-720P
cd LightX2V/scripts/wan
bash run_wan_i2v.sh

# Step-Distilled Model with LoRA
cd LightX2V/scripts/wan/distill
bash run_wan_i2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_i2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_i2v_distill_lora_4step_cfg.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_i2v_distill_model_4step_cfg.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_i2v_distill_fp8_4step_cfg.sh
```
Using a single H100 GPU, the runtime and peak VRAM usage (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Wan2.1-I2V-14B-480P model: Total Cost 276.509557 seconds; 38906MiB
2. Step-Distilled Model with LoRA: Total Cost 85.217124 seconds; 38556MiB
3. Step-Distilled Model with Merged LoRA: Total Cost 79.389818 seconds; 38556MiB
4. Step-Distilled + FP8 Quantization Model: Total Cost 68.124415 seconds; 23400MiB

#### 1.3 Multi-GPU Parallel Inference

Wan2.1-T2V-14B Model
```
# Before running, replace CUDA_VISIBLE_DEVICES with the actual GPUs to be used
# At the same time, the parallel parameter in the config file also needs to be modified accordingly, satisfying cfg_p_size * seq_p_size = number of GPUs
cd LightX2V/scripts/dist_infer
bash run_wan_t2v_dist_cfg_ulysses.sh

# Step-Distilled Model with LoRA
cd LightX2V/scripts/wan/distill
bash run_wan_t2v_distill_lora_4step_cfg_ulysses.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_t2v_distill_model_4step_cfg_ulysses.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_t2v_distill_fp8_4step_cfg_ulysses.sh
```
Using 8 H100 GPUs, the runtime and peak VRAM usage per GPU (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Wan2.1-I2V-14B-480P model: Total Cost 131.553567 seconds; 44624MiB
2. Step-Distilled Model with LoRA: Total Cost 38.337339 seconds; 43850MiB
3. Step-Distilled Model with Merged LoRA: Total Cost 29.021527 seconds; 43470MiB
4. Step-Distilled + FP8 Quantization Model: Total Cost 26.409164 seconds; 30162MiB

Wan2.1-I2V-14B Model
```
# Switch model_path and config_json to experience Wan2.1-I2V-14B-480P and Wan2.1-I2V-14B-720P
cd LightX2V/scripts/dist_infer
bash run_wan_i2v_dist_cfg_ulysses.sh

# Step-Distilled Model with LoRA
cd LightX2V/scripts/wan/distill
bash run_wan_i2v_distill_lora_4step_cfg_ulysses.sh

# Step-Distilled Model with Merged LoRA
bash run_wan_i2v_distill_model_4step_cfg_ulysses.sh

# Step-Distilled + FP8 Quantization Model
bash run_wan_i2v_distill_fp8_4step_cfg_ulysses.sh
```
Using 8 H100 GPUs, the runtime and peak VRAM usage per GPU (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Wan2.1-I2V-14B-480P model: Total Cost 116.455286 seconds; 49668MiB
2. Step-Distilled Model with LoRA: Total Cost 45.899316 seconds; 48854MiB
3. Step-Distilled Model with Merged LoRA: Total Cost 33.472992 seconds; 48674MiB
4. Step-Distilled + FP8 Quantization Model: Total Cost 30.796211 seconds; 33328MiB

Detailed Explanation

The content of run_wan_t2v_dist_cfg_ulysses.sh script is as follows:
```
#!/bin/bash

# set path firstly
lightx2v_path=
model_path=

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# set environment variables
source ${lightx2v_path}/scripts/base/base.sh

torchrun --nproc_per_node=8 -m lightx2v.infer \
--model_cls wan2.1 \
--task t2v \
--model_path $model_path \
--config_json ${lightx2v_path}/configs/dist_infer/wan_t2v_dist_cfg_ulysses.json \
--prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage." \
--negative_prompt "镜头晃动，色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走" \
--save_result_path ${lightx2v_path}/save_results/output_lightx2v_wan_t2v.mp4

```
`export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7` means using GPUs 0-7, a total of 8 GPUs

`source ${lightx2v_path}/scripts/base/base.sh` sets some basic environment variables

`torchrun --nproc_per_node=8 -m lightx2v.infer` means using torchrun to start multi-GPU inference, launching 8 processes, each process binds to 1 GPU

`--model_cls wan2.1` means using the wan2.1 model

`--task t2v` means using the t2v task, corresponding to i2v when running the Wan2.1-I2V-14B model

`--model_path` means the path to the model

`--config_json` means the path to the configuration file

`--prompt` means the prompt text

`--negative_prompt` means the negative prompt text

`--save_result_path` means the path to save the results

Since different models have their own characteristics, the `config_json` file will contain more detailed configuration parameters for the corresponding model. The content of `config_json` files differs for different models

The content of wan_t2v_dist_cfg_ulysses.json is as follows:
```
{
    "infer_steps": 50,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 8,
    "enable_cfg": true,
    "cpu_offload": false,
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
}

```
`infer_steps` means the number of inference steps

`target_video_length` means the frame count of the target video (for the wan2.1 model, fps=16, so target_video_length=81 represents a video duration of 5 seconds)

`target_height` means the height of the target video

`target_width` means the width of the target video

`self_attn_1_type`, `cross_attn_1_type`, `cross_attn_2_type` mean the types of operators for the three attention layers inside the wan2.1 model. Here we use flash_attn3, which is limited to Hopper architecture GPUs (H100, H20, etc.). Other GPUs can use flash_attn2 as a substitute

`enable_cfg` means whether to enable cfg. When set to true, it will perform inference twice, first with positive prompts and second with negative prompts, which can achieve better results but will increase inference time. If the model has already done CFG distillation, this can be set to false

`cpu_offload` means whether to enable CPU offload. Enabling CPU offload can reduce VRAM usage. If CPU offload is enabled, you need to add `"offload_granularity": "model"`, which indicates the unloading granularity - unload by entire model module. After enabling, you can use `watch -n 1 nvidia-smi` to observe VRAM usage.

`parallel` means parallel parameter settings. DiT supports two types of parallel attention mechanisms: Ulysses and Ring, and also supports CFG parallel inference. Parallel inference can significantly reduce inference time and reduce VRAM overhead for each GPU. Here we use CFG + Ulysses parallelism, corresponding to seq_p_size*cfg_p_size=8 for 8-GPU configuration

The content of wan_t2v_distill_lora_4step_cfg_ulysses.json is as follows:
```
{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 5,
    "enable_cfg": false,
    "cpu_offload": false,
    "denoising_step_list": [1000, 750, 500, 250],
    "lora_configs": [
      {
        "path": "lightx2v/Wan2.1-Distill-Loras/wan2.1_t2v_14b_lora_rank64_lightx2v_4step.safetensors",
        "strength": 1.0
      }
    ],
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
  }

```
`infer_steps` means the number of inference steps. Here we use the distilled model, with inference steps distilled to 4

`denoising_step_list` means the time steps corresponding to 4 denoising steps

`lora_configs` means LoRA plugin configuration. Fill in the path to the distilled model, must be an absolute path

The content of wan_t2v_distill_model_4step_cfg_ulysses.json is as follows:
```
{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 5,
    "enable_cfg": false,
    "cpu_offload": false,
    "denoising_step_list": [1000, 750, 500, 250],
    "dit_original_ckpt": "lightx2v/Wan2.1-Distill-Models/wan2.1_t2v_14b_lightx2v_4step.safetensors",
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
}

```
`dit_original_ckpt` means the path to the distilled model after merging LoRA

The content of wan_t2v_distill_fp8_4step_cfg_ulysses.json is as follows:
```
{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "flash_attn3",
    "cross_attn_1_type": "flash_attn3",
    "cross_attn_2_type": "flash_attn3",
    "sample_guide_scale": 6,
    "sample_shift": 5,
    "enable_cfg": false,
    "cpu_offload": false,
    "denoising_step_list": [1000, 750, 500, 250],
    "dit_quantized": true,
    "dit_quantized_ckpt": "lightx2v/Wan2.1-Distill-Models/wan2.1_t2v_14b_scaled_fp8_e4m3_lightx2v_4step.safetensors",
    "dit_quant_scheme": "fp8-sgl",
    "parallel": {
        "seq_p_size": 4,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    }
}

```
`dit_quantized` means whether to enable DIT quantization. Setting to True means applying quantization processing to the model's core DIT module

`dit_quantized_ckpt` means the DIT quantization weight path, specifies the local path to the FP8 quantized DIT weight file

`dit_quant_scheme` means the DIT quantization scheme, specifies the quantization type as "fp8-sgl" (fp8-sgl means using sglang's fp8 kernel for inference)

### 2. Start Service Generation

#### 2.1 Single GPU Inference

Start the service
```
cd LightX2V/scripts/server

# Before running the following script, you need to replace lightx2v_path, model_path and config_json in the script with actual paths
# For example: lightx2v_path=/home/user/LightX2V
# For example: model_path=/home/user/models/Wan-AI/Wan2.1-T2V-14B
# For example: config_json ${lightx2v_path}/configs/wan/wan_t2v.json

# Switch model_path and config_json paths to experience different models
bash start_server.sh
```
Send requests to the service

You need to open a second terminal as a user here
```
cd LightX2V/scripts/server

# Now generate video, url = "http://localhost:8000/v1/tasks/video/"
python post.py
```
After sending the request, you can see the inference logs on the server side

#### 2.2 Single GPU Offload Inference

Modify the cpu_offload in the config file as follows to enable offload:
```
    "cpu_offload": true
```
Start the service
```
cd LightX2V/scripts/server

bash start_server.sh
```
Send requests to the service

```
cd LightX2V/scripts/server

# Now generate video, url = "http://localhost:8000/v1/tasks/video/"
python post.py
```

#### 2.3 Multi-GPU Parallel Inference

Start the service
```
cd LightX2V/scripts/server

bash start_server_cfg_ulysses.sh
```
Send requests to the service

```
cd LightX2V/scripts/server

python post.py
```
Runtime and peak VRAM usage per GPU (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Single GPU Inference: Run DiT cost 261.699812 seconds; RUN pipeline cost 261.973479 seconds; 43968MiB
2. Single GPU Offload Inference: Run DiT cost 264.445139 seconds; RUN pipeline cost 265.565198 seconds; 34932MiB
3. Multi-GPU Parallel Inference: Run DiT cost 109.518894 seconds; RUN pipeline cost 110.085543 seconds; 44624MiB

Detailed Explanation

The content of start_server.sh script is as follows
```
#!/bin/bash

# set path firstly
lightx2v_path=
model_path=

export CUDA_VISIBLE_DEVICES=0

# set environment variables
source ${lightx2v_path}/scripts/base/base.sh


# Start API server with distributed inference service
python -m lightx2v.server \
--model_cls wan2.1 \
--task t2v \
--model_path $model_path \
--config_json ${lightx2v_path}/configs/wan/wan_t2v.json \
--host 0.0.0.0 \
--port 8000

echo "Service stopped"

```
`--host 0.0.0.0` and `--port 8000` mean the service runs on port 8000 of the local machine's IP address

The content of post.py is as follows
```
import requests
from loguru import logger

if __name__ == "__main__":
    url = "http://localhost:8000/v1/tasks/video/"

    message = {
        "prompt": "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.",
        "negative_prompt": "镜头晃动，色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走",
        "image_path": "",
        "seed": 42,
        "save_result_path": "./cat_boxing_seed42.mp4",
    }

    logger.info(f"message: {message}")

    response = requests.post(url, json=message)

    logger.info(f"response: {response.json()}")

```
`url = "http://localhost:8000/v1/tasks/video/"` means sending a video generation task to port 8000 of the local machine's IP address. If it's an image generation task, the url needs to be changed to url = "http://localhost:8000/v1/tasks/image/"

`message dictionary` means the content of the request sent to the server. If `seed` is not specified, a random `seed` will be generated each time a request is sent. If `save_result_path` is not specified, a file with the same name as the task id will also be generated

### 3. Python Code Generation

#### 3.1 Single GPU Inference

```
cd LightX2V/examples/wan

# Modify model_path, save_result_path, config_json

PYTHONPATH=/home/user/LightX2V python wan_t2v.py
```
Note 1: Among the parameters set during execution, it is recommended to use the method of passing config_json to align hyperparameters with the previous script-based video generation and service-based video generation

Note 2: The PYTHONPATH path needs to be an absolute path

#### 3.2 Single GPU Offload Inference

Modify the cpu_offload in the config file as follows to enable offload:
```
    "cpu_offload": true,
    "offload_granularity": "model"
```
```
cd LightX2V/examples/wan

PYTHONPATH=/home/user/LightX2V python wan_t2v.py
```

#### 3.3 Multi-GPU Parallel Inference
```
cd LightX2V/examples/wan
# In the code, change config_json to: LightX2V/configs/dist_infer/wan_t2v_dist_cfg_ulysses.json

PROFILING_DEBUG_LEVEL=2 PYTHONPATH=/home/user/LightX2V torchrun --nproc_per_node=8 wan_t2v.py
```
Runtime and peak VRAM usage per GPU (observed with `watch -n 1 nvidia-smi`) are as follows:
1. Single GPU Inference: Run DiT cost 262.745393 seconds; RUN pipeline cost 263.279303 seconds; 44792MiB
2. Single GPU Offload Inference: Run DiT cost 263.725956 seconds; RUN pipeline cost 264.919227 seconds; 34936MiB
3. Multi-GPU Parallel Inference: Run DiT cost 113.736238 seconds; RUN pipeline cost 114.297859 seconds; 44624MiB

Detailed Explanation

The content of wan_t2v.py is as follows
```
"""
Wan2.1 text-to-video generation example.
This example demonstrates how to use LightX2V with Wan2.1 model for T2V generation.
"""

from lightx2v import LightX2VPipeline

# Initialize pipeline for Wan2.1 T2V task
pipe = LightX2VPipeline(
    model_path="/path/to/Wan2.1-T2V-14B",
    model_cls="wan2.1",
    task="t2v",
)

# Alternative: create generator from config JSON file
# pipe.create_generator(config_json="../configs/wan/wan_t2v.json")

# Create generator with specified parameters
pipe.create_generator(
    attn_mode="sage_attn2",
    infer_steps=50,
    height=480,  # Can be set to 720 for higher resolution
    width=832,  # Can be set to 1280 for higher resolution
    num_frames=81,
    guidance_scale=5.0,
    sample_shift=5.0,
)

seed = 42
prompt = "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage."
negative_prompt = "镜头晃动，色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走"
save_result_path = "/path/to/save_results/output.mp4"

pipe.generate(
    seed=seed,
    prompt=prompt,
    negative_prompt=negative_prompt,
    save_result_path=save_result_path,
)
```
Note 1: You need to modify model_path and save_result_path to actual paths

Note 2: Among the parameters set during execution, it is recommended to use the method of passing config_json to align hyperparameters with the previous script-based video generation and service-based video generation
