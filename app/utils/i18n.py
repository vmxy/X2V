"""å›½é™…åŒ–æ”¯æŒæ¨¡å—"""

import os

# é»˜è®¤è¯­è¨€
DEFAULT_LANG = os.getenv("GRADIO_LANG", "zh")

# ç¿»è¯‘å­—å…¸
TRANSLATIONS = {
    "zh": {
        "title": "ğŸ¬ LightX2V å›¾ç‰‡/è§†é¢‘ç”Ÿæˆå™¨",
        "model_config": "ğŸ—‚ï¸ æ¨¡å‹é…ç½®",
        "model_config_hint": "ğŸ’¡ **æç¤º**ï¼šè¯·ç¡®ä¿ä»¥ä¸‹æ¯ä¸ªæ¨¡å‹é€‰é¡¹è‡³å°‘æœ‰ä¸€ä¸ªå·²ä¸‹è½½âœ…çš„æ¨¡å‹å¯ç”¨ï¼Œå¦åˆ™å¯èƒ½æ— æ³•æ­£å¸¸ç”Ÿæˆè§†é¢‘ã€‚",
        "fp8_not_supported": "âš ï¸ **æ‚¨çš„è®¾å¤‡ä¸æ”¯æŒfp8æ¨ç†**ï¼Œå·²è‡ªåŠ¨éšè—åŒ…å«fp8çš„æ¨¡å‹é€‰é¡¹ã€‚",
        "model_type": "æ¨¡å‹ç±»å‹",
        "model_type_info": "Wan2.2 éœ€è¦åˆ†åˆ«æŒ‡å®šé«˜å™ªæ¨¡å‹å’Œä½å™ªæ¨¡å‹; Qwen-Image-Edit-2511 ç”¨äºå›¾ç‰‡ç¼–è¾‘(i2i); Qwen-Image-2512 ç”¨äºæ–‡æœ¬ç”Ÿæˆå›¾ç‰‡(t2i); Z-Image-Turbo ç”¨äºæ–‡æœ¬ç”Ÿæˆå›¾ç‰‡(t2i)",
        "qwen3_encoder": "ğŸ“ Qwen3 ç¼–ç å™¨",
        "scheduler": "â±ï¸ è°ƒåº¦å™¨",
        "qwen25vl_encoder": "ğŸ“ Qwen25-VL ç¼–ç å™¨",
        "task_type": "ä»»åŠ¡ç±»å‹",
        "task_type_info": "I2V: å›¾ç”Ÿè§†é¢‘, T2V: æ–‡ç”Ÿè§†é¢‘, T2I: æ–‡ç”Ÿå›¾, I2I: å›¾ç‰‡ç¼–è¾‘",
        "download_source": "ğŸ“¥ ä¸‹è½½æº",
        "download_source_info": "é€‰æ‹©æ¨¡å‹ä¸‹è½½æº",
        "diffusion_model": "ğŸ¨ Diffusionæ¨¡å‹",
        "high_noise_model": "ğŸ”Š é«˜å™ªæ¨¡å‹",
        "low_noise_model": "ğŸ”‡ ä½å™ªæ¨¡å‹",
        "text_encoder": "ğŸ“ æ–‡æœ¬ç¼–ç å™¨",
        "text_encoder_tokenizer": "ğŸ“ æ–‡æœ¬ç¼–ç å™¨ Tokenizer",
        "image_encoder": "ğŸ–¼ï¸ å›¾åƒç¼–ç å™¨",
        "image_encoder_tokenizer": "ğŸ–¼ï¸ å›¾åƒç¼–ç å™¨ Tokenizer",
        "vae": "ğŸï¸ VAEç¼–ç /è§£ç å™¨",
        "attention_operator": "âš¡ æ³¨æ„åŠ›ç®—å­",
        "attention_operator_info": "ä½¿ç”¨é€‚å½“çš„æ³¨æ„åŠ›ç®—å­åŠ é€Ÿæ¨ç†",
        "quant_operator": "âš¡çŸ©é˜µä¹˜æ³•ç®—å­",
        "quant_operator_info": "é€‰æ‹©ä½ç²¾åº¦çŸ©é˜µä¹˜æ³•ç®—å­ä»¥åŠ é€Ÿæ¨ç†",
        "input_params": "ğŸ“¥ è¾“å…¥å‚æ•°",
        "input_image": "è¾“å…¥å›¾åƒï¼ˆå¯æ‹–å…¥å¤šå¼ å›¾ç‰‡ï¼‰",
        "image_preview": "å·²ä¸Šä¼ çš„å›¾ç‰‡é¢„è§ˆ",
        "image_path": "å›¾ç‰‡è·¯å¾„",
        "prompt": "æç¤ºè¯",
        "prompt_placeholder": "æè¿°è§†é¢‘/å›¾ç‰‡å†…å®¹...",
        "negative_prompt": "è´Ÿå‘æç¤ºè¯",
        "negative_prompt_placeholder": "ä¸å¸Œæœ›å‡ºç°åœ¨è§†é¢‘/å›¾ç‰‡ä¸­çš„å†…å®¹...",
        "max_resolution": "æœ€å¤§åˆ†è¾¨ç‡",
        "max_resolution_info": "å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯è°ƒä½åˆ†è¾¨ç‡",
        "random_seed": "éšæœºç§å­",
        "infer_steps": "æ¨ç†æ­¥æ•°",
        "infer_steps_distill": "è’¸é¦æ¨¡å‹æ¨ç†æ­¥æ•°é»˜è®¤ä¸º4ã€‚",
        "infer_steps_info": "è§†é¢‘ç”Ÿæˆçš„æ¨ç†æ­¥æ•°ã€‚å¢åŠ æ­¥æ•°å¯èƒ½æé«˜è´¨é‡ä½†é™ä½é€Ÿåº¦ã€‚",
        "sample_shift": "åˆ†å¸ƒåç§»",
        "sample_shift_info": "æ§åˆ¶æ ·æœ¬åˆ†å¸ƒåç§»çš„ç¨‹åº¦ã€‚å€¼è¶Šå¤§è¡¨ç¤ºåç§»è¶Šæ˜æ˜¾ã€‚",
        "cfg_scale": "CFGç¼©æ”¾å› å­",
        "cfg_scale_info": "æ§åˆ¶æç¤ºè¯çš„å½±å“å¼ºåº¦ã€‚å€¼è¶Šé«˜ï¼Œæç¤ºè¯çš„å½±å“è¶Šå¤§ã€‚å½“å€¼ä¸º1æ—¶ï¼Œè‡ªåŠ¨ç¦ç”¨CFGã€‚",
        "enable_cfg": "å¯ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼",
        "fps": "æ¯ç§’å¸§æ•°(FPS)",
        "fps_info": "è§†é¢‘çš„æ¯ç§’å¸§æ•°ã€‚è¾ƒé«˜çš„FPSä¼šäº§ç”Ÿæ›´æµç•…çš„è§†é¢‘ã€‚",
        "num_frames": "æ€»å¸§æ•°",
        "num_frames_info": "è§†é¢‘ä¸­çš„æ€»å¸§æ•°ã€‚æ›´å¤šå¸§æ•°ä¼šäº§ç”Ÿæ›´é•¿çš„è§†é¢‘ã€‚",
        "video_duration": "è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰",
        "video_duration_info": "è§†é¢‘çš„æ—¶é•¿ï¼ˆç§’ï¼‰ã€‚å®é™…å¸§æ•° = æ—¶é•¿ Ã— FPSã€‚",
        "output_path": "è¾“å‡ºè§†é¢‘è·¯å¾„",
        "output_path_info": "å¿…é¡»åŒ…å«.mp4æ‰©å±•åã€‚å¦‚æœç•™ç©ºæˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åã€‚",
        "output_image_path": "è¾“å‡ºå›¾ç‰‡è·¯å¾„",
        "output_image_path_info": "å¿…é¡»åŒ…å«.pngæ‰©å±•åã€‚å¦‚æœç•™ç©ºæˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åã€‚",
        "output_result": "ğŸ“¤ ç”Ÿæˆçš„ç»“æœ",
        "output_image": "è¾“å‡ºå›¾ç‰‡",
        "generate_video": "ğŸ¬ ç”Ÿæˆè§†é¢‘",
        "generate_image": "ğŸ–¼ï¸ ç”Ÿæˆå›¾ç‰‡",
        "infer_steps_image_info": "å›¾ç‰‡ç¼–è¾‘çš„æ¨ç†æ­¥æ•°ï¼Œé»˜è®¤ä¸º8ã€‚",
        "aspect_ratio": "å®½é«˜æ¯”",
        "aspect_ratio_info": "é€‰æ‹©ç”Ÿæˆå›¾ç‰‡çš„å®½é«˜æ¯”",
        "model_config_hint_image": "ğŸ’¡ **æç¤º**ï¼šè¯·ç¡®ä¿ä»¥ä¸‹æ¯ä¸ªæ¨¡å‹é€‰é¡¹è‡³å°‘æœ‰ä¸€ä¸ªå·²ä¸‹è½½âœ…çš„æ¨¡å‹å¯ç”¨ï¼Œå¦åˆ™å¯èƒ½æ— æ³•æ­£å¸¸ç”Ÿæˆå›¾ç‰‡ã€‚",
        "download": "ğŸ“¥ ä¸‹è½½",
        "downloaded": "âœ… å·²ä¸‹è½½",
        "not_downloaded": "âŒ æœªä¸‹è½½",
        "download_complete": "âœ… {model_name} ä¸‹è½½å®Œæˆ",
        "download_start": "å¼€å§‹ä» {source} ä¸‹è½½ {model_name}...",
        "please_select_model": "è¯·å…ˆé€‰æ‹©æ¨¡å‹",
        "loading_models": "æ­£åœ¨åŠ è½½ Hugging Face æ¨¡å‹åˆ—è¡¨ç¼“å­˜...",
        "models_loaded": "æ¨¡å‹åˆ—è¡¨ç¼“å­˜åŠ è½½å®Œæˆ",
        "use_lora": "ä½¿ç”¨ LoRA",
        "lora": "ğŸ¨ LoRA",
        "lora_info": "é€‰æ‹©è¦ä½¿ç”¨çš„ LoRA æ¨¡å‹",
        "lora_strength": "LoRA å¼ºåº¦",
        "lora_strength_info": "æ§åˆ¶ LoRA çš„å½±å“å¼ºåº¦ï¼ŒèŒƒå›´ 0-10",
        "high_noise_lora": "ğŸ”Š é«˜å™ªæ¨¡å‹ LoRA",
        "high_noise_lora_info": "é€‰æ‹©é«˜å™ªæ¨¡å‹ä½¿ç”¨çš„ LoRA",
        "high_noise_lora_strength": "é«˜å™ªæ¨¡å‹ LoRA å¼ºåº¦",
        "high_noise_lora_strength_info": "æ§åˆ¶é«˜å™ªæ¨¡å‹ LoRA çš„å½±å“å¼ºåº¦ï¼ŒèŒƒå›´ 0-10",
        "low_noise_lora": "ğŸ”‡ ä½å™ªæ¨¡å‹ LoRA",
        "low_noise_lora_info": "é€‰æ‹©ä½å™ªæ¨¡å‹ä½¿ç”¨çš„ LoRA",
        "low_noise_lora_strength": "ä½å™ªæ¨¡å‹ LoRA å¼ºåº¦",
        "low_noise_lora_strength_info": "æ§åˆ¶ä½å™ªæ¨¡å‹ LoRA çš„å½±å“å¼ºåº¦ï¼ŒèŒƒå›´ 0-10",
    },
    "en": {
        "title": "ğŸ¬ LightX2V Image/Video Generator",
        "model_config": "ğŸ—‚ï¸ Model Configuration",
        "model_config_hint": "ğŸ’¡ **Tip**: Please ensure at least one downloaded âœ… model is available for each model option below, otherwise video generation may fail.",
        "fp8_not_supported": "âš ï¸ **Your device does not support fp8 inference**, fp8 model options have been automatically hidden.",
        "model_type": "Model Type",
        "model_type_info": "Wan2.2 requires separate high-noise and low-noise models; Qwen-Image-Edit-2511 is for image editing (i2i); Qwen-Image-2512 is for text-to-image (t2i); Z-Image-Turbo is for text-to-image (t2i)",
        "qwen3_encoder": "ğŸ“ Qwen3 Encoder",
        "scheduler": "â±ï¸ Scheduler",
        "qwen25vl_encoder": "ğŸ“ Qwen25-VL Encoder",
        "task_type": "Task Type",
        "task_type_info": "I2V: Image-to-Video, T2V: Text-to-Video, T2I: Text-to-Image, I2I: Image Editing",
        "download_source": "ğŸ“¥ Download Source",
        "download_source_info": "Select model download source",
        "diffusion_model": "ğŸ¨ Diffusion Model",
        "high_noise_model": "ğŸ”Š High Noise Model",
        "low_noise_model": "ğŸ”‡ Low Noise Model",
        "text_encoder": "ğŸ“ Text Encoder",
        "text_encoder_tokenizer": "ğŸ“ Text Encoder Tokenizer",
        "image_encoder": "ğŸ–¼ï¸ Image Encoder",
        "image_encoder_tokenizer": "ğŸ–¼ï¸ Image Encoder Tokenizer",
        "vae": "ğŸï¸ VAE Encoder/Decoder",
        "attention_operator": "âš¡ Attention Operator",
        "attention_operator_info": "Use appropriate attention operator to accelerate inference",
        "quant_operator": "âš¡ Matrix Multiplication Operator",
        "quant_operator_info": "Select low-precision matrix multiplication operator to accelerate inference",
        "input_params": "ğŸ“¥ Input Parameters",
        "input_image": "Input Image (drag multiple images)",
        "image_preview": "Uploaded Image Preview",
        "image_path": "Image Path",
        "prompt": "Prompt",
        "prompt_placeholder": "Describe video/image content...",
        "negative_prompt": "Negative Prompt",
        "negative_prompt_placeholder": "Content you don't want in the video/image...",
        "max_resolution": "Max Resolution",
        "max_resolution_info": "Reduce resolution if VRAM is insufficient",
        "random_seed": "Random Seed",
        "infer_steps": "Inference Steps",
        "infer_steps_distill": "Distill model inference steps default to 4.",
        "infer_steps_info": "Number of inference steps for video generation. More steps may improve quality but reduce speed.",
        "sample_shift": "Sample Shift",
        "sample_shift_info": "Control the degree of sample distribution shift. Higher values indicate more obvious shift.",
        "cfg_scale": "CFG Scale",
        "cfg_scale_info": "Control the influence strength of prompts. Higher values mean stronger prompt influence. When value is 1, CFG is automatically disabled.",
        "enable_cfg": "Enable Classifier-Free Guidance",
        "fps": "Frames Per Second (FPS)",
        "fps_info": "Frames per second of the video. Higher FPS produces smoother videos.",
        "num_frames": "Total Frames",
        "num_frames_info": "Total number of frames in the video. More frames produce longer videos.",
        "video_duration": "Video Duration (seconds)",
        "video_duration_info": "Duration of the video in seconds. Actual frames = duration Ã— FPS.",
        "output_path": "Output Video Path",
        "output_path_info": "Must include .mp4 extension. If left empty or using default value, a unique filename will be automatically generated.",
        "output_image_path": "Output Image Path",
        "output_image_path_info": "Must include .png extension. If left empty or using default value, a unique filename will be automatically generated.",
        "output_result": "ğŸ“¤ Generated Result",
        "output_image": "Output Image",
        "generate_video": "ğŸ¬ Generate Video",
        "generate_image": "ğŸ–¼ï¸ Generate Image",
        "infer_steps_image_info": "Number of inference steps for image editing, default is 8.",
        "aspect_ratio": "Aspect Ratio",
        "aspect_ratio_info": "Select the aspect ratio for generated images",
        "model_config_hint_image": "ğŸ’¡ **Tip**: Please ensure at least one downloaded âœ… model is available for each model option below, otherwise image generation may fail.",
        "download": "ğŸ“¥ Download",
        "downloaded": "âœ… Downloaded",
        "not_downloaded": "âŒ Not Downloaded",
        "download_complete": "âœ… {model_name} download complete",
        "download_start": "Starting to download {model_name} from {source}...",
        "please_select_model": "Please select a model first",
        "loading_models": "Loading Hugging Face model list cache...",
        "models_loaded": "Model list cache loaded",
        "use_lora": "Use LoRA",
        "lora": "ğŸ¨ LoRA",
        "lora_info": "Select LoRA model to use",
        "lora_strength": "LoRA Strength",
        "lora_strength_info": "Control LoRA influence strength, range 0-10",
        "high_noise_lora": "ğŸ”Š High Noise Model LoRA",
        "high_noise_lora_info": "Select high noise model LoRA to use",
        "high_noise_lora_strength": "High Noise Model LoRA Strength",
        "high_noise_lora_strength_info": "Control high noise model LoRA influence strength, range 0-10",
        "low_noise_lora": "ğŸ”‡ Low Noise Model LoRA",
        "low_noise_lora_info": "Select low noise model LoRA to use",
        "low_noise_lora_strength": "Low Noise Model LoRA Strength",
        "low_noise_lora_strength_info": "Control low noise model LoRA influence strength, range 0-10",
    },
}


def t(key: str, lang: str = None) -> str:
    """è·å–ç¿»è¯‘æ–‡æœ¬"""
    if lang is None:
        lang = DEFAULT_LANG

    if lang not in TRANSLATIONS:
        lang = "zh"

    return TRANSLATIONS[lang].get(key, key)


def set_language(lang: str):
    """è®¾ç½®è¯­è¨€"""
    global DEFAULT_LANG
    if lang in TRANSLATIONS:
        DEFAULT_LANG = lang
        os.environ["GRADIO_LANG"] = lang
