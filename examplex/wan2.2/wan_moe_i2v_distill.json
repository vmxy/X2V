{
    "infer_steps": 4,
    "target_video_length": 81,
    "text_len": 512,
    "target_height": 480,
    "target_width": 832,
    "self_attn_1_type": "sage_attn3",
    "cross_attn_1_type": "sage_attn3",
    "cross_attn_2_type": "sage_attn3",
    "sample_guide_scale": [
        3.5,
        3.5
    ],
    "dit_quantized": true,
    "dit_quant_scheme": "fp8-torchao",
    "high_noise_quantized_ckpt": "/data/ai-models/lightx2v/wan2.2/Wan2.2-Distill-Models/wan2.2_i2v_A14b_high_noise_scaled_fp8_e4m3_lightx2v_4step_1030.safetensors",
    "low_noise_quantized_ckpt":"/data/ai-models/lightx2v/wan2.2/Wan2.2-Distill-Models/wan2.2_i2v_A14b_low_noise_scaled_fp8_e4m3_lightx2v_4step.safetensors",
    "high_noise_quantized_ckptx": "/data/ai-models/lightx2v/wan2.2/Wan2.2-Distill-Models/wan2.2_i2v_A14b_high_noise_scaled_nvfp4_lightx2v_4step_0130.safetensors",
    "low_noise_quantized_ckptx":"/data/ai-models/lightx2v/wan2.2/Wan2.2-Distill-Models/wan2.2_i2v_A14b_low_noise_scaled_nvfp4_lightx2v_4step_0130.safetensors",
    "t5_original_ckpt_xxxxxx": "/data/ai-models/commons/models_t5_umt5-xxl-enc-bf16.pth",
    "tensor_parallel": true,
    "parallel": {
        "seq_p_size": 1,
        "seq_p_attn_type": "ulysses",
        "cfg_p_size": 2
    },
    "sample_shift": 5.0,
    "enable_cfg": false,
    "cpu_offload": true,
    "offload_granularity": "block",
    "t5_cpu_offload": true,
    "vae_cpu_offload": true,
    "use_image_encoder": false,
    "boundary_step_index": 2,
    "denoising_step_list": [
        1000,
        750,
        500,
        250
    ]
}
